{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x7f740dd65970>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qibullet import SimulationManager\n",
    "from qibullet import NaoVirtual , NaoFsr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu  in use\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = \"cuda\" \n",
    "else:\n",
    "  device = \"cpu\"\n",
    "print(device, \" in use\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nao Bullet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Launch Simulation Environment\n",
    "vis = True\n",
    "simulation_manager = SimulationManager()\n",
    "nao_sim = simulation_manager.launchSimulation(gui=False, auto_step=True)\n",
    "#nao_sim_vis = simulation_manager.launchSimulation(gui=False, auto_step=True)\n",
    "simulation_manager.setGravity(nao_sim, [0.0, 0.0, -9.81])\n",
    "nao = None\n",
    "\n",
    "# Utility Functions\n",
    "# Joints to monitor\n",
    "legJoints = [ \"LHipRoll\", \"LHipPitch\", \"LKneePitch\", \"LAnklePitch\", \"LAnkleRoll\",\n",
    "              \"RHipRoll\", \"RHipPitch\", \"RKneePitch\", \"RAnklePitch\", \"RAnkleRoll\"]\n",
    "\n",
    "min_values_tensor = torch.tensor([-0.37, -1.53, -0.09, -1.18, -0.39 , -0.79, -1.53, -0.10, -1.18, -0.76], dtype=torch.float32)\n",
    "max_values_tensor = torch.tensor([ 0.79, 0.48, 2.11, 0.92, 0.76,  0.37, 0.48, 2.12, 0.93, 0.39], dtype=torch.float32)\n",
    "\n",
    "min_values = [-0.38, -1.539, -0.099,\t-1.19 , -0.399 ,\t-0.792, -1.536, -0.105, -1.188, -0.769]\n",
    "max_values = [ 0.795, 0.488, 2.118,0.923, 0.77,  0.38, 0.49, 2.13, 0.95,0.399]\n",
    "bucket_width = 0.1\n",
    "# Compute the number of buckets for each element of the array\n",
    "num_buckets = np.ceil((np.array(max_values) - np.array(min_values)) / bucket_width).astype(int)\n",
    "\n",
    "# Create the list of bucket edges for each element of the array\n",
    "bucket_edges_list = [np.linspace(start=min_value, stop=max_value, num=num_bucket+1) for min_value, max_value, num_bucket in zip(min_values, max_values, num_buckets)]\n",
    "\n",
    "\n",
    "nao_prev_position_X = 0\n",
    "nao_prev_position_Y = 0\n",
    "\n",
    "prev_weight_cntr = 0\n",
    "def env_Reward():\n",
    "    global prev_weight_cntr\n",
    "    x, y, z = nao.getPosition()\n",
    "    terminated = False\n",
    "    delX = 5*(nao_prev_position_X - x )\n",
    "    delY = 5*(nao_prev_position_Y - y )\n",
    "    weight = -(nao.getTotalFsrValues(NaoFsr.LFOOT) + nao.getTotalFsrValues(NaoFsr.RFOOT) )\n",
    "\n",
    "    # if robot fallen down\n",
    "    if weight == 0: \n",
    "        reward = -1\n",
    "        prev_weight_cntr += 1\n",
    "    else:\n",
    "        reward = delX\n",
    "        prev_weight_cntr = 0\n",
    "\n",
    "    if prev_weight_cntr == 5:\n",
    "        terminated = True\n",
    "        prev_weight_cntr = 0\n",
    "    return reward, terminated\n",
    "\n",
    "def env_state():\n",
    "    return nao.getAnglesPosition(legJoints)\n",
    "    \n",
    "def env_action(angles, speed):\n",
    "    nao.setAngles(legJoints, angles ,speed)\n",
    "\n",
    "def env_itrm_step(itr):\n",
    "    for _ in range(itr):\n",
    "        simulation_manager.stepSimulation(nao_sim)\n",
    "\n",
    "def env_reset():\n",
    "    global nao_prev_position_X , nao_prev_position_Y\n",
    "    global nao\n",
    "    simulation_manager.resetSimulation(nao_sim)\n",
    "    simulation_manager.setGravity(nao_sim, [0.0, 0.0, -9.81])\n",
    "    nao = simulation_manager.spawnNao( \n",
    "    nao_sim,\n",
    "    translation=[0, 0, 0],quaternion=[0, 0, 0, 1],\n",
    "    spawn_ground_plane=True)\n",
    "    env_itrm_step(100)\n",
    "    nao_prev_position_X, nao_prev_position_Y, _ = nao.getPosition()\n",
    "\n",
    "def env_stop():\n",
    "    simulation_manager.stopSimulation(nao_sim)\n",
    "    # wait for limited iterations \n",
    "\n",
    "def env_step(angles):\n",
    "    env_action(angles, 0.5)\n",
    "    #env_itrm_step(50)\n",
    "    time.sleep(0.3)\n",
    "    reward, terminated = env_Reward()\n",
    "    next_state = env_state()\n",
    "    return next_state, reward, terminated\n",
    "\n",
    "def env_Q_state():\n",
    "    my_array = env_state()\n",
    "    # Define a function to assign each element of the array to a bucket\n",
    "    def assign_bucket(value):\n",
    "        index = np.where(value == my_array)[0][0]  # Find the index of the value in the array\n",
    "        bucket_edges = bucket_edges_list[index]   # Get the corresponding bucket edges\n",
    "        bucket_index = np.searchsorted(bucket_edges, value, side='right') - 1  # Find the bucket index\n",
    "        return bucket_index\n",
    "\n",
    "    my_array = np.array(my_array)\n",
    "\n",
    "    bucket_indices = [assign_bucket(value) for value in my_array]\n",
    "    return bucket_indices\n",
    "    \n",
    "env_reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Agent \n",
    "class walk_Agent:\n",
    "  def __init__(self, observation_space, action_space , epsilon, epsilon_decay):\n",
    "    self.observation_space = observation_space\n",
    "    self.action_space = action_space\n",
    "    self.epsilon = epsilon\n",
    "    self.epsilon_decay = epsilon_decay\n",
    "    self.epsilon_array = list()\n",
    "    self.epsilon_min = 0.35\n",
    "\n",
    "  def step(self, obs, policy, Q):\n",
    "    self.epsilon = max ( self.epsilon * self.epsilon_decay , self.epsilon_min)\n",
    "    self.epsilon_array.append(self.epsilon)\n",
    "\n",
    "    #obs = np.array(obs)\n",
    "    #obs = list( obs.flatten())\n",
    "    if policy == \"e-greedy\":\n",
    "      if np.random.uniform() > self.epsilon:\n",
    "        next_obs = []\n",
    "        act = []\n",
    "        for i in range(self.observation_space):\n",
    "          idx = np.argmax( [ Q[3*i] , Q[3*i + 1], Q[3*i + 2] ])\n",
    "          if idx == 0:\n",
    "            next_obs.append( obs[i] + 0.1)\n",
    "            act.append(0)\n",
    "          elif idx == 1:\n",
    "            next_obs.append( obs[i] )\n",
    "            act.append(1)\n",
    "          elif idx == 2:\n",
    "            next_obs.append( obs[i] - 0.1)\n",
    "            act.append(2)\n",
    "          \n",
    "      else:\n",
    "        act = []\n",
    "        next_obs = []\n",
    "        for elem in obs:\n",
    "          elem += random.choice([ 0.1, 0, -0.1])\n",
    "          if elem == 0.1:\n",
    "            act.append(0)\n",
    "          elif elem == 0:\n",
    "            act.append(1)\n",
    "          else:\n",
    "            act.append(2)\n",
    "          next_obs.append(elem)\n",
    "        \n",
    "      return next_obs , act\n",
    "\n",
    "    elif policy == \"random\":\n",
    "      return np.random.choice(self.action_space.n)\n",
    "\n",
    "    elif policy == \"optimal\":\n",
    "      return np.argmax(Q)\n",
    "  \n",
    "  def give_epsilon_history(self):\n",
    "    return self.epsilon_array"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation :  [[ 5. 17.  0. 13.  3.  6. 17.  0. 13.  9.]]\n",
      "Q Values from Action Value :  [[ 7.5337780e-01 -1.7502315e+00 -1.1084392e+00  2.6178451e+00\n",
      "  -3.0137923e+00  2.6336172e+00  6.4629722e-01  1.6161565e+00\n",
      "  -1.5202831e+00  2.9719837e+00  1.2684149e+00  3.1629076e+00\n",
      "   2.1339784e+00 -1.4091963e+00  5.4755741e-01 -3.1435776e+00\n",
      "   1.7057283e+00 -2.1219053e+00 -6.3218683e-01  7.8435099e-01\n",
      "  -1.8975660e+00  1.4886105e+00 -6.9488382e+00  1.5880821e+00\n",
      "  -7.3648386e+00 -8.9088053e-01  5.6162828e-01 -2.2720780e+00\n",
      "  -3.9497465e-03 -3.6295075e+00]]\n",
      "Q Values from Target Action :  [[ 7.5337780e-01 -1.7502315e+00 -1.1084392e+00  2.6178451e+00\n",
      "  -3.0137923e+00  2.6336172e+00  6.4629722e-01  1.6161565e+00\n",
      "  -1.5202831e+00  2.9719837e+00  1.2684149e+00  3.1629076e+00\n",
      "   2.1339784e+00 -1.4091963e+00  5.4755741e-01 -3.1435776e+00\n",
      "   1.7057283e+00 -2.1219053e+00 -6.3218683e-01  7.8435099e-01\n",
      "  -1.8975660e+00  1.4886105e+00 -6.9488382e+00  1.5880821e+00\n",
      "  -7.3648386e+00 -8.9088053e-01  5.6162828e-01 -2.2720780e+00\n",
      "  -3.9497465e-03 -3.6295075e+00]]\n"
     ]
    }
   ],
   "source": [
    "### Paramters\n",
    "epsilon = 1\n",
    "gamma = 0.9\n",
    "epsilon_decay = 0.999\n",
    "\n",
    "observation_space = 10\n",
    "action_space= 20\n",
    "\n",
    "## Environment Initialisation\n",
    "agent = walk_Agent(observation_space, action_space, epsilon, epsilon_decay)\n",
    "\n",
    "env_reset()\n",
    "terminated = False\n",
    "\n",
    "# BEGIN_YOUR_CODE\n",
    "observation = env_Q_state()\n",
    "observation = np.array([observation], dtype='float32')\n",
    "\n",
    "# Define Action Value model\n",
    "class DQN_AV(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network_arch = nn.Sequential(\n",
    "            nn.Linear(10, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 30)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.network_arch(x)\n",
    "        return out\n",
    "\n",
    "AV = DQN_AV().to(device)\n",
    "#print(model)\n",
    "\n",
    "\n",
    "# Define Target Action model\n",
    "class DQN_TA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network_arch = nn.Sequential(\n",
    "            nn.Linear(10, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 30)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.network_arch(x)\n",
    "        return out\n",
    "\n",
    "TA = DQN_TA().to(device)\n",
    "#print(model)\n",
    "\n",
    "TA.load_state_dict(AV.state_dict())\n",
    "\n",
    "input = torch.from_numpy(observation)\n",
    "print(\"Observation : \" , observation)\n",
    "\n",
    "out = AV(input)\n",
    "print(\"Q Values from Action Value : \", out.detach().numpy())\n",
    "\n",
    "out = TA(input)\n",
    "print(\"Q Values from Target Action : \", out.detach().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/hri/cse546_proj/nao_bullet/dqn.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/hri/cse546_proj/nao_bullet/dqn.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=128'>129</a>\u001b[0m out_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mempty(\u001b[39m10\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/hri/cse546_proj/nao_bullet/dqn.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=129'>130</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m (\u001b[39m10\u001b[39m):\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/hri/cse546_proj/nao_bullet/dqn.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=130'>131</a>\u001b[0m   out_tensor[i] \u001b[39m=\u001b[39m state_action_values[\u001b[39m3\u001b[39;49m\u001b[39m*\u001b[39;49mi \u001b[39m+\u001b[39;49m action_arr[i]]\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/hri/cse546_proj/nao_bullet/dqn.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=132'>133</a>\u001b[0m expected_state_action_values \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(exp, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/hri/cse546_proj/nao_bullet/dqn.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=134'>135</a>\u001b[0m \u001b[39m## Gradient descent on Q\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/hri/cse546_proj/nao_bullet/dqn.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=135'>136</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/hri/cse546_proj/nao_bullet/dqn.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=136'>137</a>\u001b[0m \u001b[39m# Compute Huber loss\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'list'"
     ]
    }
   ],
   "source": [
    "Q = np.zeros((10, 30))\n",
    "QT = np.zeros((10, 30))\n",
    "\n",
    "### Paramters\n",
    "epsilon = 1\n",
    "gamma = 0.9\n",
    "epsilon_decay = 0.99999\n",
    "\n",
    "policy = \"e-greedy\"\n",
    "\n",
    "D = list()\n",
    "avg_reward = 0\n",
    "Cum_reward_arr = list()\n",
    "no_of_timestep = list()\n",
    "\n",
    "## Environment Initialisation\n",
    "agent = walk_Agent(observation_space, action_space, epsilon, epsilon_decay)\n",
    "\n",
    "\n",
    "## Reset\n",
    "env_reset()\n",
    "obs = env_Q_state()\n",
    "obs = np.array([obs], dtype='float32')\n",
    "terminated = False\n",
    "\n",
    "\n",
    "## Parameters\n",
    "miniBatch = 32\n",
    "C = 10\n",
    "episodes = 10\n",
    "M = 500\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = optim.AdamW(AV.parameters(), lr= learning_rate, amsgrad=True)\n",
    "\n",
    "#fig, ax = plt.subplots()\n",
    "#ax.set_xlabel(\"iteration\")\n",
    "#ax.set_ylabel(\"Reward\")\n",
    "\n",
    "x = 0\n",
    "for m in range(M):\n",
    "  \n",
    "  #print(\"--------------------------------------------------\")\n",
    "  #print(\"Iteration: \" , m, \" is in process\")\n",
    "  x = len(Cum_reward_arr)\n",
    "  for episode in range(episodes):\n",
    "    itr = 0\n",
    "    total_reward = 0\n",
    "    with torch.no_grad():\n",
    "      while not terminated:\n",
    "        itr = itr +1\n",
    "        \n",
    "        obs = np.array(obs)\n",
    "        obs = list( obs.flatten())\n",
    "        Q = AV(torch.tensor(obs, dtype=torch.float))\n",
    "        \n",
    "        ## Take Action\n",
    "        action , act = agent.step(obs, policy, list( Q.numpy() ))\n",
    "        \n",
    "        ## Observe reward and state\n",
    "        new_obs, reward, terminated = env_step(list(action))\n",
    "\n",
    "        ## Store in D \n",
    "        D.append([obs, action, act , reward, new_obs, terminated])\n",
    "\n",
    "        ## Update State\n",
    "        obs = new_obs\n",
    "        \n",
    "        ## Cumulative reward\n",
    "        total_reward += reward\n",
    "        \n",
    "        if(terminated):\n",
    "            no_of_timestep.append(itr)\n",
    "            break\n",
    "      \n",
    "      env_reset()\n",
    "      obs = env_Q_state()\n",
    "      obs = np.array([obs], dtype='float32')\n",
    "      terminated = False\n",
    "      Cum_reward_arr.append(total_reward)\n",
    "      avg_reward =  0.05 * total_reward + (1-0.05) * avg_reward\n",
    "\n",
    "\n",
    "  totol_loop = len(D)//miniBatch\n",
    "  \n",
    "  \n",
    "# --------------------------------------------------#\n",
    "  for k in range(totol_loop):\n",
    "    ## Sample random from miniBatch\n",
    "    err = 0\n",
    "    state_arr = list()\n",
    "    action_arr = list()\n",
    "    reward_arr = list()\n",
    "    next_arr= list()\n",
    "    exp = list()\n",
    "\n",
    "    for i in range(miniBatch):\n",
    "      idx = random.randint(1, len(D) -1)\n",
    "      st, a, act,  r, st1, terminated = D[idx]\n",
    "\n",
    "      state_arr.append(st)\n",
    "      action_arr.append(act)\n",
    "      #reward_arr.append(r)\n",
    "\n",
    "      if terminated:\n",
    "        y = r * torch.ones(10)\n",
    "      else:\n",
    "        QT = TA(torch.tensor(st1, dtype=torch.float)) # 30\n",
    "        grouped_tensor = QT.view(-1, 3)\n",
    "        # Find maximum value in each group\n",
    "        max_values = grouped_tensor.max(dim=1).values\n",
    "        y = r + gamma*max_values\n",
    "\n",
    "      exp.append(y)  # 10 - 32\n",
    "    \n",
    "    state_arr = torch.tensor(state_arr , dtype=torch.float)\n",
    "    \n",
    "    #action_arr = torch.tensor(action_arr, dtype=torch.int64)\n",
    "    #reward_arr = torch.tensor(reward_arr, dtype=torch.float)\n",
    "\n",
    "    #state_arr = state_arr.view(32)\n",
    "    #action_arr = action_arr.view(32, 10)\n",
    "\n",
    "    #print(state_arr.shape)\n",
    "    #print(action_arr.shape)\n",
    "\n",
    "    state_action_values = AV(state_arr) # 30 - 32\n",
    "\n",
    "    out_tensor = torch.empty(10)\n",
    "    for i in range (10):\n",
    "      out_tensor[i] = state_action_values[3*i + action_arr[i]]\n",
    "    \n",
    "    expected_state_action_values = torch.tensor(exp, dtype=torch.float)\n",
    "\n",
    "    ## Gradient descent on Q\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(out_tensor, expected_state_action_values)\n",
    "    \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(AV.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    ## Update after Every C step\n",
    "    if k%C == 0:\n",
    "      TA.load_state_dict(AV.state_dict())\n",
    "  \n",
    "  if (m+1)% 10 == 0:\n",
    "    plt.plot(Cum_reward_arr)\n",
    "    plt.plot(avg_reward)\n",
    "    plt.pause(0.001)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.11887248450722195,\n",
       " 0.12742912178680993,\n",
       " -0.09231670170576244,\n",
       " 0.0860346155204499,\n",
       " -0.11101797276179945,\n",
       " -0.11902312619501815,\n",
       " 0.1272078410658313,\n",
       " -0.09237377788730246,\n",
       " 0.0864412977180074,\n",
       " 0.111324775703716]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = torch.sign(x)\n",
    "        return x\n",
    "\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., -1., -1., -1., -1.,  1., -1.,  1., -1.,  1.]],\n",
       "       grad_fn=<SignBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randn(1, 10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THis is directly learning policy not value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AV(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x)) \n",
    "        x = self.custom_activation(x)\n",
    "        return x\n",
    "\n",
    "    def custom_activation(self, x):\n",
    "        x = torch.where(x < -0.33, torch.tensor(-1.0), x)\n",
    "        x = torch.where((x >= -0.33) & (x <= 0.33), torch.tensor(0.0), x)\n",
    "        x = torch.where(x > 0.33, torch.tensor(1.0), x)\n",
    "        return x\n",
    "\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<WhereBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nao_bullet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
