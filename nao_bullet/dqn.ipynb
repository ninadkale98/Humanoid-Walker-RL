{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x7f6ae4236820>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qibullet import SimulationManager\n",
    "from qibullet import NaoVirtual , NaoFsr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu  in use\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = \"cuda\" \n",
    "else:\n",
    "  device = \"cpu\"\n",
    "print(device, \" in use\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nao Bullet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=VMware, Inc.\n",
      "GL_RENDERER=llvmpipe (LLVM 10.0.0, 256 bits)\n",
      "GL_VERSION=3.3 (Core Profile) Mesa 20.0.8\n",
      "GL_SHADING_LANGUAGE_VERSION=3.30\n",
      "pthread_getconcurrency()=0\n",
      "Version = 3.3 (Core Profile) Mesa 20.0.8\n",
      "Vendor = VMware, Inc.\n",
      "Renderer = llvmpipe (LLVM 10.0.0, 256 bits)\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n",
      "ven = VMware, Inc.\n",
      "ven = VMware, Inc.\n"
     ]
    }
   ],
   "source": [
    "# Launch Simulation Environment\n",
    "vis = True\n",
    "simulation_manager = SimulationManager()\n",
    "nao_sim = simulation_manager.launchSimulation(gui=vis, auto_step=True)\n",
    "#nao_sim_vis = simulation_manager.launchSimulation(gui=False, auto_step=True)\n",
    "simulation_manager.setGravity(nao_sim, [0.0, 0.0, -9.81])\n",
    "nao = None\n",
    "\n",
    "# Utility Functions\n",
    "# Joints to monitor\n",
    "legJoints = [ \"LHipRoll\", \"LHipPitch\", \"LKneePitch\", \"LAnklePitch\", \"LAnkleRoll\",\n",
    "              \"RHipRoll\", \"RHipPitch\", \"RKneePitch\", \"RAnklePitch\", \"RAnkleRoll\"]\n",
    "\n",
    "min_values_tensor = torch.tensor([-0.37, -1.53, -0.09, -1.18, -0.39 , -0.79, -1.53, -0.10, -1.18, -0.76], dtype=torch.float32)\n",
    "max_values_tensor = torch.tensor([ 0.79, 0.48, 2.11, 0.92, 0.76,  0.37, 0.48, 2.12, 0.93, 0.39], dtype=torch.float32)\n",
    "\n",
    "min_values = [-0.38, -1.539, -0.099,\t-1.19 , -0.399 ,\t-0.792, -1.536, -0.105, -1.188, -0.769]\n",
    "max_values = [ 0.795, 0.488, 2.118,0.923, 0.77,  0.38, 0.49, 2.13, 0.95,0.399]\n",
    "bucket_width = 0.05\n",
    "\n",
    "num_buckets = np.ceil((np.array(max_values) - np.array(min_values)) / bucket_width).astype(int)\n",
    "bucket_edges_list = [np.linspace(start=min_value, stop=max_value, num=num_bucket+1) for min_value, max_value, num_bucket in zip(min_values, max_values, num_buckets)]\n",
    "\n",
    "nao_prev_position_X = 0\n",
    "nao_prev_position_Y = 0\n",
    "\n",
    "prev_weight_cntr = 0\n",
    "def env_Reward():\n",
    "    global prev_weight_cntr\n",
    "    x, y, z = nao.getPosition()\n",
    "    terminated = False\n",
    "    delX = 5*(x - nao_prev_position_X  )\n",
    "    delY = 5*(y - nao_prev_position_Y  )\n",
    "    weight = -(nao.getTotalFsrValues(NaoFsr.LFOOT) + nao.getTotalFsrValues(NaoFsr.RFOOT) )\n",
    "\n",
    "    # if robot fallen down\n",
    "    if weight == 0: \n",
    "        reward = 0\n",
    "        prev_weight_cntr += 1\n",
    "    else:\n",
    "        reward = delX\n",
    "        prev_weight_cntr = 0\n",
    "\n",
    "    if prev_weight_cntr == 5:\n",
    "        terminated = True\n",
    "        reward = - 1\n",
    "        prev_weight_cntr = 0\n",
    "    return reward, terminated\n",
    "\n",
    "def env_state():\n",
    "    return nao.getAnglesPosition(legJoints)\n",
    "    \n",
    "def env_action(angles, speed):\n",
    "    nao.setAngles(legJoints, angles ,speed)\n",
    "\n",
    "def env_itrm_step(itr):\n",
    "    for _ in range(itr):\n",
    "        simulation_manager.stepSimulation(nao_sim)\n",
    "\n",
    "def env_reset():\n",
    "    global nao_prev_position_X , nao_prev_position_Y\n",
    "    global nao\n",
    "    simulation_manager.resetSimulation(nao_sim)\n",
    "    simulation_manager.setGravity(nao_sim, [0.0, 0.0, -9.81])\n",
    "    nao = simulation_manager.spawnNao( \n",
    "    nao_sim,\n",
    "    translation=[0, 0, 0],quaternion=[0, 0, 0, 1],\n",
    "    spawn_ground_plane=True)\n",
    "    env_itrm_step(100)\n",
    "    time.sleep(2)\n",
    "    nao_prev_position_X, nao_prev_position_Y, _ = nao.getPosition()\n",
    "\n",
    "def env_stop():\n",
    "    simulation_manager.stopSimulation(nao_sim)\n",
    "    # wait for limited iterations \n",
    "\n",
    "def env_step(angles):\n",
    "    env_action(angles, 0.5)\n",
    "    #env_itrm_step(50)\n",
    "    time.sleep(0.5)\n",
    "    reward, terminated = env_Reward()\n",
    "    next_state = env_state()\n",
    "    return next_state, reward, terminated\n",
    "\n",
    "def env_Q_state():\n",
    "    my_array = env_state()\n",
    "    # Define a function to assign each element of the array to a bucket\n",
    "    def assign_bucket(value):\n",
    "        index = np.where(value == my_array)[0][0]  # Find the index of the value in the array\n",
    "        bucket_edges = bucket_edges_list[index]   # Get the corresponding bucket edges\n",
    "        bucket_index = np.searchsorted(bucket_edges, value, side='right') - 1  # Find the bucket index\n",
    "        return bucket_index\n",
    "\n",
    "    my_array = np.array(my_array)\n",
    "\n",
    "    bucket_indices = [assign_bucket(value) for value in my_array]\n",
    "    return bucket_indices\n",
    "    \n",
    "env_reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numActiveThreads = 0\n",
      "stopping threads\n",
      "Thread with taskId 0 exiting\n",
      "Thread TERMINATED\n",
      "destroy semaphore\n",
      "semaphore destroyed\n",
      "destroy main semaphore\n",
      "main semaphore destroyed\n",
      "finished\n",
      "numActiveThreads = 0\n",
      "btShutDownExampleBrowser stopping threads\n",
      "Thread with taskId 0 exiting\n",
      "Thread TERMINATED\n",
      "destroy semaphore\n",
      "semaphore destroyed\n",
      "destroy main semaphore\n",
      "main semaphore destroyed\n"
     ]
    }
   ],
   "source": [
    "env_stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Agent \n",
    "class walk_Agent:\n",
    "  def __init__(self, observation_space, action_space , epsilon, epsilon_decay):\n",
    "    self.observation_space = observation_space\n",
    "    self.action_space = action_space\n",
    "    self.epsilon = epsilon\n",
    "    self.epsilon_decay = epsilon_decay\n",
    "    self.epsilon_array = list()\n",
    "    self.epsilon_min = 0.35\n",
    "\n",
    "  def step(self, obs, policy, Q):\n",
    "    self.epsilon = max ( self.epsilon * self.epsilon_decay , self.epsilon_min)\n",
    "    self.epsilon_array.append(self.epsilon)\n",
    "\n",
    "    if policy == \"e-greedy\":\n",
    "\n",
    "      if np.random.uniform() > self.epsilon:\n",
    "    \n",
    "        next_obs = []\n",
    "        act = []\n",
    "        for i in range(self.observation_space):\n",
    "          idx = np.argmax( [ Q[3*i] , Q[3*i + 1], Q[3*i + 2] ])\n",
    "          if idx == 0:\n",
    "            next_obs.append( obs[i] + bucket_width)\n",
    "            act.append(0)\n",
    "\n",
    "          elif idx == 1:\n",
    "            next_obs.append( obs[i] )\n",
    "            act.append(1)\n",
    "\n",
    "          elif idx == 2:\n",
    "            next_obs.append( obs[i] - bucket_width)\n",
    "            act.append(2)\n",
    "          \n",
    "      else:\n",
    "\n",
    "        act = []\n",
    "        next_obs = []\n",
    "        for elem in obs:\n",
    "\n",
    "          rnd_act = random.choice([ 0.1, 0, -0.1])\n",
    "          \n",
    "          if rnd_act == 0.1:\n",
    "            next_obs.append(elem + bucket_width)\n",
    "            act.append(0)\n",
    "          \n",
    "          elif rnd_act == 0:\n",
    "            next_obs.append(elem)\n",
    "            act.append(1)\n",
    "          \n",
    "          else:\n",
    "            next_obs.append(elem - bucket_width)\n",
    "            act.append(2)\n",
    "        \n",
    "      return next_obs , act\n",
    "\n",
    "    elif policy == \"random\":\n",
    "      return np.random.choice(self.action_space.n)\n",
    "\n",
    "    elif policy == \"optimal\":\n",
    "      return np.argmax(Q)\n",
    "  \n",
    "  def give_epsilon_history(self):\n",
    "    return self.epsilon_array"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation :  [[ 0.11933001  0.12696193 -0.09232651  0.08701695 -0.11081475 -0.11916356\n",
      "   0.12730536 -0.0923344   0.0867851   0.11116954]]\n",
      "Q Values from Action Value :  [[-0.11874117 -0.10125618 -0.01200833  0.02571335  0.10584147 -0.06745173\n",
      "   0.06017124 -0.00129769 -0.06409799 -0.0805539  -0.0114157   0.02199012\n",
      "  -0.07894025  0.00808182  0.03094829 -0.01753118  0.03220341  0.04193366\n",
      "  -0.0711465   0.07060177 -0.02010316 -0.00327753  0.03432719  0.12484895\n",
      "   0.13039258 -0.02490954 -0.07420933 -0.01702947 -0.04905571 -0.05769888]]\n",
      "Q Values from Target Action :  [[-0.11874117 -0.10125618 -0.01200833  0.02571335  0.10584147 -0.06745173\n",
      "   0.06017124 -0.00129769 -0.06409799 -0.0805539  -0.0114157   0.02199012\n",
      "  -0.07894025  0.00808182  0.03094829 -0.01753118  0.03220341  0.04193366\n",
      "  -0.0711465   0.07060177 -0.02010316 -0.00327753  0.03432719  0.12484895\n",
      "   0.13039258 -0.02490954 -0.07420933 -0.01702947 -0.04905571 -0.05769888]]\n"
     ]
    }
   ],
   "source": [
    "### Paramters\n",
    "epsilon = 1\n",
    "gamma = 0.9\n",
    "epsilon_decay = 0.999\n",
    "\n",
    "observation_space = 10\n",
    "action_space= 20\n",
    "\n",
    "## Environment Initialisation\n",
    "agent = walk_Agent(observation_space, action_space, epsilon, epsilon_decay)\n",
    "\n",
    "env_reset()\n",
    "terminated = False\n",
    "\n",
    "# BEGIN_YOUR_CODE\n",
    "observation = env_state()\n",
    "observation = np.array([observation], dtype='float32')\n",
    "\n",
    "# Define Action Value model\n",
    "class DQN_AV(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network_arch = nn.Sequential(\n",
    "            nn.Linear(10, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 30)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.network_arch(x)\n",
    "        return out\n",
    "\n",
    "AV = DQN_AV().to(device)\n",
    "#print(model)\n",
    "\n",
    "\n",
    "# Define Target Action model\n",
    "class DQN_TA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network_arch = nn.Sequential(\n",
    "            nn.Linear(10, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 30)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.network_arch(x)\n",
    "        return out\n",
    "\n",
    "TA = DQN_TA().to(device)\n",
    "#print(model)\n",
    "\n",
    "TA.load_state_dict(AV.state_dict())\n",
    "\n",
    "input = torch.from_numpy(observation)\n",
    "print(\"Observation : \" , observation)\n",
    "\n",
    "out = AV(input)\n",
    "print(\"Q Values from Action Value : \", out.detach().numpy())\n",
    "\n",
    "out = TA(input)\n",
    "print(\"Q Values from Target Action : \", out.detach().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "Error in the setAngles parameters",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/qibullet/robot_virtual.py:144\u001b[0m, in \u001b[0;36mRobotVirtual.setAngles\u001b[0;34m(self, joint_names, joint_values, percentage_speeds)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(joint_names) \u001b[39m==\u001b[39m\\\n\u001b[1;32m    145\u001b[0m         \u001b[39mlen\u001b[39m(joint_values) \u001b[39m==\u001b[39m\\\n\u001b[1;32m    146\u001b[0m         \u001b[39mlen\u001b[39m(percentage_speeds)\n\u001b[1;32m    148\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[1;32m    149\u001b[0m         speed \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mand\u001b[39;00m speed \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39mfor\u001b[39;00m speed \u001b[39min\u001b[39;00m percentage_speeds)\n",
      "\u001b[0;31mAssertionError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m/home/hri/cse546_proj/nao_bullet/dqn.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/hri/cse546_proj/nao_bullet/dqn.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m new_state_req , action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mstep(obs, policy, \u001b[39mlist\u001b[39m( Q\u001b[39m.\u001b[39mnumpy() )) \u001b[39m# new_state_req are angles and action will 10*1 array of -1 , 1 , 0\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/hri/cse546_proj/nao_bullet/dqn.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39m## Observe reward and state\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/hri/cse546_proj/nao_bullet/dqn.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m new_obs, reward, terminated \u001b[39m=\u001b[39m env_step(\u001b[39mlist\u001b[39;49m(new_state_req))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/hri/cse546_proj/nao_bullet/dqn.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39m## Store in D \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/hri/cse546_proj/nao_bullet/dqn.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m D\u001b[39m.\u001b[39mappend([obs, action, reward, new_obs, terminated])\n",
      "\u001b[1;32m/home/hri/cse546_proj/nao_bullet/dqn.ipynb Cell 12\u001b[0m in \u001b[0;36menv_step\u001b[0;34m(angles)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/hri/cse546_proj/nao_bullet/dqn.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39menv_step\u001b[39m(angles):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/hri/cse546_proj/nao_bullet/dqn.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m     env_action(angles, \u001b[39m0.5\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/hri/cse546_proj/nao_bullet/dqn.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m     \u001b[39m#env_itrm_step(50)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/hri/cse546_proj/nao_bullet/dqn.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.5\u001b[39m)\n",
      "\u001b[1;32m/home/hri/cse546_proj/nao_bullet/dqn.ipynb Cell 12\u001b[0m in \u001b[0;36menv_action\u001b[0;34m(angles, speed)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/hri/cse546_proj/nao_bullet/dqn.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39menv_action\u001b[39m(angles, speed):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/hri/cse546_proj/nao_bullet/dqn.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m     nao\u001b[39m.\u001b[39;49msetAngles(legJoints, angles ,speed)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/qibullet/nao_virtual.py:274\u001b[0m, in \u001b[0;36mNaoVirtual.setAngles\u001b[0;34m(self, joint_names, joint_values, percentage_speed)\u001b[0m\n\u001b[1;32m    271\u001b[0m         values\u001b[39m.\u001b[39mextend(finger_values)\n\u001b[1;32m    272\u001b[0m         speeds\u001b[39m.\u001b[39mextend([speed]\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(finger_names))\n\u001b[0;32m--> 274\u001b[0m RobotVirtual\u001b[39m.\u001b[39;49msetAngles(\n\u001b[1;32m    275\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    276\u001b[0m     names,\n\u001b[1;32m    277\u001b[0m     values,\n\u001b[1;32m    278\u001b[0m     speeds)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/qibullet/robot_virtual.py:152\u001b[0m, in \u001b[0;36mRobotVirtual.setAngles\u001b[0;34m(self, joint_names, joint_values, percentage_speeds)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[1;32m    149\u001b[0m         speed \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mand\u001b[39;00m speed \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39mfor\u001b[39;00m speed \u001b[39min\u001b[39;00m percentage_speeds)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m:\n\u001b[0;32m--> 152\u001b[0m     \u001b[39mraise\u001b[39;00m pybullet\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mError in the setAngles parameters\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    154\u001b[0m \u001b[39mfor\u001b[39;00m joint_name, joint_value, percentage_speed \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\n\u001b[1;32m    155\u001b[0m         joint_names,\n\u001b[1;32m    156\u001b[0m         joint_values,\n\u001b[1;32m    157\u001b[0m         percentage_speeds):\n\u001b[1;32m    159\u001b[0m     \u001b[39mif\u001b[39;00m isNan(joint_value):\n",
      "\u001b[0;31merror\u001b[0m: Error in the setAngles parameters"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Paramters\n",
    "epsilon = 1\n",
    "gamma = 0.9\n",
    "epsilon_decay = 0.999\n",
    "\n",
    "observation_space = 10\n",
    "action_space= 10\n",
    "\n",
    "policy = \"e-greedy\"\n",
    "\n",
    "D = list()\n",
    "avg_reward = 0\n",
    "Cum_reward_arr = list()\n",
    "no_of_timestep = list()\n",
    "\n",
    "## Environment Initialisation\n",
    "agent = walk_Agent(observation_space, action_space, epsilon, epsilon_decay)\n",
    "\n",
    "## Reset\n",
    "env_reset()\n",
    "obs = env_state()\n",
    "obs = np.array([obs], dtype='float32')\n",
    "terminated = False\n",
    "\n",
    "\n",
    "## Parameters\n",
    "miniBatch = 4\n",
    "C = 2\n",
    "episodes = 10\n",
    "M = 500\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = optim.AdamW(AV.parameters(), lr= learning_rate, amsgrad=True)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"iteration\")\n",
    "ax.set_ylabel(\"Reward\")\n",
    "\n",
    "for m in range(M):\n",
    "  \n",
    "  D = list()\n",
    "  for episode in range(episodes):\n",
    "\n",
    "    itr = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "      # Collect experiences\n",
    "      while not terminated:\n",
    "\n",
    "        itr = itr +1\n",
    "        \n",
    "        #obs = env_Q_state()\n",
    "        #obs = np.array([obs], dtype='float32')\n",
    "        #obs = torch.from_numpy(obs)\n",
    "        \n",
    "        obs = env_state() #np.array(obs)\n",
    "        #obs = list( obs.flatten())\n",
    "        obs = np.array([obs], dtype='float32')\n",
    "        Q = AV(torch.tensor(obs, dtype=torch.float)) # 1d array of size 30\n",
    "        \n",
    "        ## Take Action\n",
    "        new_state_req , action = agent.step(obs, policy, list( Q.numpy() )) # new_state_req are angles and action will 10*1 array of -1 , 1 , 0\n",
    "        \n",
    "        ## Observe reward and state\n",
    "        new_obs, reward, terminated = env_step(list(new_state_req))\n",
    "\n",
    "        ## Store in D \n",
    "        D.append([obs, action, reward, new_obs, terminated])\n",
    "\n",
    "        ## Update State\n",
    "        #obs = new_obs\n",
    "        \n",
    "        ## Cumulative reward\n",
    "        total_reward += reward\n",
    "        \n",
    "        if(terminated):\n",
    "            no_of_timestep.append(itr)\n",
    "            break\n",
    "      \n",
    "      env_reset()\n",
    "      obs = env_Q_state()\n",
    "      obs = np.array([obs], dtype='float32')\n",
    "      terminated = False\n",
    "      Cum_reward_arr.append(total_reward)\n",
    "      avg_reward =  0.05 * total_reward + (1-0.05) * avg_reward\n",
    "\n",
    "  totol_loop = len(D)//miniBatch\n",
    "  \n",
    "# --------------------------------------------------\n",
    "  for k in range(totol_loop):\n",
    "    \n",
    "    err = 0\n",
    "    exp = list()\n",
    "    pred = list()\n",
    "\n",
    "    ## Sample random from miniBatch\n",
    "    for i in range(miniBatch):\n",
    "      idx = random.randint(1, len(D) -1)\n",
    "      st, a,  r, st1, terminated = D[idx]\n",
    "\n",
    "      if terminated:\n",
    "        Q_exp = r * torch.ones(10)\n",
    "      else:\n",
    "        QT = TA(torch.tensor(st1, dtype=torch.float))\n",
    "        QT = QT.view(-1, 3)\n",
    "        max_values = QT.max(dim=1).values\n",
    "        Q_exp = r + gamma*max_values\n",
    "\n",
    "      Q = AV(torch.tensor(st, dtype=torch.float))\n",
    "      Q = Q.view(-1, 3)\n",
    "      idx = torch.tensor([ [a[0]], [a[1]], [a[2]], [a[3]], [a[4]], [a[5]], [a[6]], [a[7]], [a[8]], [a[9]]  ])\n",
    "      Q_pred = torch.gather(Q, 1, idx).squeeze()\n",
    "\n",
    "      exp.append(Q_exp)\n",
    "      pred.append(Q_pred)\n",
    "    \n",
    "    #print(exp)\n",
    "    #print(pred)\n",
    "\n",
    "    expected = exp \n",
    "    predicted = pred \n",
    "\n",
    "    loss = torch.nn.functional.mse_loss(expected[0], predicted[0])\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    #torch.nn.utils.clip_grad_value_(AV.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "    ## Update after Every C step\n",
    "    if k%C == 0:\n",
    "      TA.load_state_dict(AV.state_dict())\n",
    "  \n",
    "  if (m)% 5 == 0:\n",
    "    plt.plot(Cum_reward_arr)\n",
    "    plt.plot(no_of_timestep)\n",
    "    plt.pause(0.001)\n",
    "\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Index tensor must have the same number of dimensions as input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/hri/cse546_proj/nao_bullet/dqn.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/hri/cse546_proj/nao_bullet/dqn.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m4\u001b[39m,\u001b[39m5\u001b[39m,\u001b[39m6\u001b[39m,\u001b[39m7\u001b[39m,\u001b[39m8\u001b[39m,\u001b[39m9\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/hri/cse546_proj/nao_bullet/dqn.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/hri/cse546_proj/nao_bullet/dqn.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m torch\u001b[39m.\u001b[39;49mgather(x, \u001b[39m1\u001b[39;49m, torch\u001b[39m.\u001b[39;49mtensor([\u001b[39m0\u001b[39;49m,\u001b[39m1\u001b[39;49m,\u001b[39m0\u001b[39;49m]))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Index tensor must have the same number of dimensions as input tensor"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1,2,3,4,5,6,7,8,9])\n",
    "x = x.view(-1, 3)\n",
    "torch.gather(x, 1, torch.tensor([0,1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Index tensor must have the same number of dimensions as input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/hri/cse546_proj/nao_bullet/dqn.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/hri/cse546_proj/nao_bullet/dqn.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39;49mgather(x, \u001b[39m1\u001b[39;49m, torch\u001b[39m.\u001b[39;49mtensor([\u001b[39m0\u001b[39;49m,\u001b[39m1\u001b[39;49m,\u001b[39m0\u001b[39;49m]))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Index tensor must have the same number of dimensions as input tensor"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 1],\n",
       "        [4, 5, 4],\n",
       "        [7, 8, 7]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1,2,3,4,5,6,7,8,9])\n",
    "x = x.view(-1, 3)\n",
    "idx = torch.tensor([0,1,0])\n",
    "idx = idx.repeat(x.size()[0], 1)  # repeat idx for each row of x\n",
    "torch.gather(x, 1, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2308, -0.1738, -0.3404,  0.1829, -0.1477,  0.0149,  0.1514,  0.2285,\n",
       "        -0.0203, -0.0408])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor( [ [ 0.2308,  0.1351,  0.1310],\n",
    "                    [ 0.3405, -0.1738,  0.0268],\n",
    "                    [-0.3404, -0.1835, -0.0311],\n",
    "                    [ 0.1829,  0.1309,  0.0402],\n",
    "                    [ 0.0356, -0.1477,  0.0833],\n",
    "                    [ 0.0149, -0.1418, -0.1558],\n",
    "                    [ 0.1514, -0.0096, -0.0639],\n",
    "                    [-0.0182,  0.2285, -0.2660],\n",
    "                    [-0.0203, -0.1473,  0.1908],\n",
    "                    [ 0.2329, -0.0408, -0.0517] ])\n",
    "\n",
    "idx = torch.tensor([[0],[1],[0],[0],[1],[0],[0],[1],[0],[1] ])\n",
    "torch.gather(x, 1, idx).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.11860806640630592,\n",
       " 0.12884769051249473,\n",
       " -0.09229718608198108,\n",
       " 0.08667567864562271,\n",
       " -0.11086502016204691,\n",
       " -0.11894127989195233,\n",
       " 0.12729024957515186,\n",
       " -0.09222518471334772,\n",
       " 0.08818516055705587,\n",
       " 0.11231001838456815]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nao.getAnglesPosition(legJoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 33, 0, 25, 5, 13, 33, 0, 25, 18]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_Q_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = torch.sign(x)\n",
    "        return x\n",
    "\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., -1., -1., -1., -1.,  1., -1.,  1., -1.,  1.]],\n",
       "       grad_fn=<SignBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randn(1, 10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THis is directly learning policy not value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AV(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x)) \n",
    "        x = self.custom_activation(x)\n",
    "        return x\n",
    "\n",
    "    def custom_activation(self, x):\n",
    "        x = torch.where(x < -0.33, torch.tensor(-1.0), x)\n",
    "        x = torch.where((x >= -0.33) & (x <= 0.33), torch.tensor(0.0), x)\n",
    "        x = torch.where(x > 0.33, torch.tensor(1.0), x)\n",
    "        return x\n",
    "\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<WhereBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nao_bullet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
